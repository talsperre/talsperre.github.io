<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> OpenAI Native Image Generation - Generative Reality, Generative UI, and more | Shashank Srikanth </title> <meta name="author" content="Shashank Srikanth"> <meta name="description" content="Generative Reality - The next step in native image generation"> <meta name="keywords" content="Metaflow, Shashank Srikanth, tech, blog, academic-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://talsperre.github.io/blog/2025/generated-reality/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/img-comparison-slider@8.0.6/dist/styles.min.css" integrity="sha256-3qTIuuUWIFnnU3LpQMjqiXc0p09rvd0dmj+WkpQXSR8=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/swiper@11.0.5/swiper-bundle.min.css" integrity="sha256-yUoNxsvX+Vo8Trj3lZ/Y5ZBf8HlBFsB6Xwm7rH75/9E=" crossorigin="anonymous"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Shashank</span> Srikanth </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">GitHub Profile </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">OpenAI Native Image Generation - Generative Reality, Generative UI, and more</h1> <p class="post-meta"> Created in February 16, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/llms"> <i class="fa-solid fa-hashtag fa-sm"></i> llms</a>   <a href="/blog/tag/ar"> <i class="fa-solid fa-hashtag fa-sm"></i> ar</a>   <a href="/blog/tag/openai"> <i class="fa-solid fa-hashtag fa-sm"></i> openai</a>   ·   <a href="/blog/category/tech"> <i class="fa-solid fa-tag fa-sm"></i> tech</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>A short post on native image generation in LLMs, OpenAI’s GPT-4o release and Studio Ghibli trend, and applications of native image generation like “Generative UI” and “Generative Reality”.</p> <h2 id="openai-native-image-generation"><strong>OpenAI Native Image Generation</strong></h2> <p>OpenAI <a href="https://openai.com/index/introducing-4o-image-generation/" rel="external nofollow noopener" target="_blank">announced</a> native image generation in <code class="language-plaintext highlighter-rouge">GPT-4o</code> on March 25th, 2025. Immediately, after the announcement, Machine Learning <a href="https://grok.com/share/bGVnYWN5_9177404c-2e12-4246-abc4-5b422ec48a9a" rel="external nofollow noopener" target="_blank">TPOT</a> (This Part of Twitter) was flooded with tweets about the image generation capabilities of this model, particularly its ability to generate a version of the provided image in the style of <a href="https://en.wikipedia.org/wiki/Studio_Ghibli" rel="external nofollow noopener" target="_blank">Studio Ghibli</a> animations, initially popularized by Hayao Miyazaki and Isao Takahata.</p> <div class="container"> <div class="row justify-content-center"> <div class="col-md-6"> <div class="jekyll-twitter-plugin"> <blockquote class="twitter-tweet"> <p lang="en" dir="ltr">tremendous alpha right now in sending your wife photos of yall converted to studio ghibli anime <a href="https://t.co/FROszdFSfN" rel="external nofollow noopener" target="_blank">pic.twitter.com/FROszdFSfN</a></p>— Grant Slatton (@GrantSlatton) <a href="https://twitter.com/GrantSlatton/status/1904631016356274286?ref_src=twsrc%5Etfw" rel="external nofollow noopener" target="_blank">March 25, 2025</a> </blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> </div> <div class="col-md-6"> <div class="jekyll-twitter-plugin"> <blockquote class="twitter-tweet"> <p lang="en" dir="ltr">Iconic movie scenes, Studio Ghibli style. This is so much fun! <a href="https://t.co/LwjkNjcEV9" rel="external nofollow noopener" target="_blank">pic.twitter.com/LwjkNjcEV9</a></p>— Mufaddal Durbar (@MDurbar) <a href="https://twitter.com/MDurbar/status/1904872441899339963?ref_src=twsrc%5Etfw" rel="external nofollow noopener" target="_blank">March 26, 2025</a> </blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> </div> </div> </div> <p>The “offending” tweet (left one), which opened the floodgates, was from a user named <a href="https://x.com/GrantSlatton/status/1904631016356274286" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">@GrantSlatton</code></a>. Even though, generating ghiblified versions of images is fun, these native image generation models are capable of much more than that. They can potentially remove watermarks from images, add/remove subjects from images, help with interior design and more. Taking it even further, a more advanced version of such models can operate in real-time, altering the world we see and perceive around us in real-time, leading to the term “Generative Reality”. The later sections of this post will explore this concept in more detail. What’s interesting is that despite all the hype about AI art and AI generated images, the most popular use case right now seems to be modifying existing images of friends, family, and pets in various styles. Imitation is the sincerest form of flattery, and the fact that people are using these models to add filters on top of their own photos, rather than creating entirely new images suggests two things: True creativity is still exceptionally rare and very valuable, and people still find immense joy in personal connections and memories.</p> <p><em>This newfound capability raises a natural question: what exactly is native image generation, and how does it differ from earlier approaches? Let’s dive into the technical details.</em></p> <h3 id="what-is-native-image-generation">What is Native Image Generation?</h3> <p>Earlier image generation models like <code class="language-plaintext highlighter-rouge">DALL-E</code>, <code class="language-plaintext highlighter-rouge">CLIP</code>, and <code class="language-plaintext highlighter-rouge">Imagen</code> relied on diffusion models, Vision Transformers, or Generative Adversarial Networks (does anyone still remember GANs?). In contrast, newer models like <code class="language-plaintext highlighter-rouge">GPT-4o</code>, <code class="language-plaintext highlighter-rouge">Grok 3</code>, and <code class="language-plaintext highlighter-rouge">gemini-2.0-flash-exp-image-generation</code> (yes, that’s really the name 🤦‍️) are truly multimodal. These models can generate images and audio in an autoregressive manner, much like they generate text. For instance, <code class="language-plaintext highlighter-rouge">GPT-4o</code> produces images pixel by pixel, predicting the next pixel one at a time. Multimodal models treat text, image pixels, and audio waveforms as different tokens, training jointly across all three modalities. The image and GIF below illustrate the inputs for a multimodal model and how autoregressive text generation works in practice for unimodal models like <code class="language-plaintext highlighter-rouge">GPT-2</code>.</p> <div class="row justify-content-center w-100"> <div class="col-md-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post3/multimodal-llms-480.webp 480w,/assets/img/post3/multimodal-llms-800.webp 800w,/assets/img/post3/multimodal-llms-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/post3/multimodal-llms.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figcaption class="figure-caption text-center mt-2">Multimodal Large Language Model architecture</figcaption> </div> <div class="col-md-8"> <figure class="figure"> <img src="/assets/img/post3/gpt-2-autoregression-2.gif" alt="Autoregressive generation" class="img-fluid rounded z-depth-1" onclick="this.classList.toggle('zoomed')"> <figcaption class="figure-caption text-center mt-2">Autoregressive text generation process in GPT-2 <a href="https://jalammar.github.io/illustrated-gpt2/" rel="external nofollow noopener" target="_blank">[*] </a></figcaption> </figure> </div> </div> <p><br></p> <p>The main advantage of this approach is efficiency: we no longer need specialized systems for different modalities, streamlining both training and inference. Additionally, these models leverage cross-modal relationships, enhancing their contextual understanding of scenes.</p> <h3 id="how-does-it-really-work">How Does It Really Work?</h3> <p>Multimodal LLMs fall into two main categories:</p> <ul> <li>Models that process multiple input modalities (e.g., images, audio, text) but only generate text as output, such as <a href="https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/" rel="external nofollow noopener" target="_blank">LLaMA 3.2</a>.</li> <li>Models that can also generate images or audio as output, like <code class="language-plaintext highlighter-rouge">GPT-4o</code> and <code class="language-plaintext highlighter-rouge">Grok 3</code>.</li> </ul> <h4 id="deeper-dive-into-multi-modal-models">Deeper dive into multi-modal models</h4> <div class="row justify-content-center w-100"> <div class="col-md-12"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post3/multimodal-llm-input-480.webp 480w,/assets/img/post3/multimodal-llm-input-800.webp 800w,/assets/img/post3/multimodal-llm-input-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/post3/multimodal-llm-input.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figcaption class="figure-caption text-center mt-2">Processing image using VIT for multi-modal LLMs <a href="https://sebastianraschka.com/blog/2024/understanding-multimodal-llms.html" rel="external nofollow noopener" target="_blank">[*] </a></figcaption> </div> </div> <p>The mechanism of processing multimodal inputs is roughly similar in both these types of models but the output generation is different. The image below shows how a multi-modal LLM can process an image as well as text together to generate a response. These multi-modal models typically process an image into a smaller chunks of size <code class="language-plaintext highlighter-rouge">16 x 16</code> or <code class="language-plaintext highlighter-rouge">32 x 32</code> pixels, in a left to right, top to bottom manner. These chunks are fed in a sequential manner to another model, typically a Vision Transformer (ViT), which processes these image chunks and generates a representation for each chunk. These intermediate representations are then fed into a linear projection layer, which resizes the image representations to the same dimensionality as the input text embeddings, and also ensures that the generated image embeddings are in the same “latent space” as the text embeddings. This alignment is done by training the model, specifically the projection layers on a large dataset of text-image pairs, after the base LLM has finished training. Models from different research groups use varying approaches for training, especially regarding which layers to freeze and which to update, but it common to only update the linear projection layer and image encoder during training. For instance, see this snippet from the LLama 3.2 blog post:</p> <blockquote> <p>To add image input support, we trained a set of adapter weights that integrate the pre-trained image encoder into the pre-trained language model. The adapter consists of a series of cross-attention layers that feed image encoder representations into the language model. We trained the adapter on text-image pairs to align the image representations with the language representations. During adapter training, we also updated the parameters of the image encoder, but intentionally did not update the language-model parameters. By doing that, we keep all the text-only capabilities intact, providing developers a drop-in replacement for Llama 3.1 models.</p> </blockquote> <p>This <a href="https://sebastianraschka.com/blog/2024/understanding-multimodal-llms.html" rel="external nofollow noopener" target="_blank">blog post</a> by Sebastian Raschka provides a more technical deep dive into how multi-modal models are trained, and the state-of-the-art models in this space as of late</p> <ol> <li>However, the blog post does not cover the generation/decoding process for multimodal models like <code class="language-plaintext highlighter-rouge">GPT-4o</code> and <code class="language-plaintext highlighter-rouge">Grok 3</code>, which not only process multimodal inputs, but are also capable of generating images in an autoregressive manner. The concept of generating images in an autoregressive manner is not new, and has been explored as far back as the 2016s in papers like <a href="https://arxiv.org/abs/1601.06759" rel="external nofollow noopener" target="_blank">PixelRNN</a>. PixelRNN simply used an RNN to predict the next pixel in an image, given the previous pixels. However, there have been several improvements to such models in recent years, with most of the improvements coming in the way that i) images are encoded (CLIP/VQ-VAE) and, ii) modifying the process of autoregressive generation itself from sequential visual token generation in a raster-scan order, to more complex generation processes like Visual AutoRegressive modeling (VAR), which is autoregressive generation of images as coarse-to-fine “next-scale prediction” or “next-resolution prediction”. Visual AutoRegressive modeling was introduced in the 2024 NeurIPS Best Paper award-winning paper <a href="https://arxiv.org/pdf/2404.02905" rel="external nofollow noopener" target="_blank">“Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction”</a>.</li> </ol> <ul> <li>The model first generates a low-resolution image starting from a \(1 \times 1\) token map, and then progressively increases the resolution by making the transformer based model to predict the higher resolution map.</li> <li>Each higher resolution map is conditioned on the previous lower resolution map, thus making the generation process autoregressive.</li> <li>Importantly, the authors show empirical validation of te scaling laws and zero-shot generalization potential of the VAR model, which is markedly similar to those of other LLMs.</li> </ul> <div class="row justify-content-center w-100"> <div class="col-md-12"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post3/var-auto-regressive-models-480.webp 480w,/assets/img/post3/var-auto-regressive-models-800.webp 800w,/assets/img/post3/var-auto-regressive-models-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/post3/var-auto-regressive-models.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figcaption class="figure-caption text-center mt-2">A Visual AutoRegressive model generates images in a coarse-to-fine manner <a href="https://arxiv.org/pdf/2404.02905" rel="external nofollow noopener" target="_blank">[*]</a></figcaption> </div> </div> <p><br></p> <p>The diagram above shows the process of VAR generation as described in the paper. This is not the only way to generate images in an autoregressive manner, and it isn’t clear if <code class="language-plaintext highlighter-rouge">GPT-4o</code> or other native image generation models like <code class="language-plaintext highlighter-rouge">Gemini 2.0 Pro Experimental</code> and <code class="language-plaintext highlighter-rouge">Grok 3</code> use similar techniques. There have been some speculations that the GPT-4o model performs autoregressive generation of images in a raster-scan order (left to right, top to bottom), with generation happening at all scales simultaneously. Another speculation is that there are two separate models, <code class="language-plaintext highlighter-rouge">GPT-4o</code> generates tokens in the image latent space, and a separate diffusion based decoder generates the actual image - see tweets below. Given the artifacts being generated in the ChatGPT UI like top-down generation, blurry intermediates, etc., I believe the latter speculation is more likely.</p> <div class="container"> <div class="row justify-content-center"> <div class="col-md-6"> <div class="jekyll-twitter-plugin"> <blockquote class="twitter-tweet"> <p lang="en" dir="ltr">Okay my working hypothesis for 4o image generation is that it is jointly performing autoregressive inference (raster scanline order) on an image pyramid at all scales simultaneously. <a href="https://t.co/WMoidIuvN3" rel="external nofollow noopener" target="_blank">https://t.co/WMoidIuvN3</a></p>— Jon Barron (@jon_barron) <a href="https://twitter.com/jon_barron/status/1905143840572583974?ref_src=twsrc%5Etfw" rel="external nofollow noopener" target="_blank">March 27, 2025</a> </blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> </div> <div class="col-md-6"> <div class="jekyll-twitter-plugin"> <blockquote class="twitter-tweet"> <p lang="en" dir="ltr">How would gpt-4o image generation work? Speculation:<br><br>- gpt-4o generates visual tokens, and the diffusion decoder decodes them to pixel space. <br>- Not just diffusion but Rolling Diffusion-like group-wise diffusion decoder, top-&gt;bottom ordering. <a href="https://t.co/4uIv8m7aIq" rel="external nofollow noopener" target="_blank">pic.twitter.com/4uIv8m7aIq</a></p>— Sangyun Lee (@sang_yun_lee) <a href="https://twitter.com/sang_yun_lee/status/1905411685499691416?ref_src=twsrc%5Etfw" rel="external nofollow noopener" target="_blank">March 28, 2025</a> </blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> </div> </div> </div> <h3 id="is-any-of-this-new"><strong>Is any of this new?</strong></h3> <p><strong>The resounding answer is no.</strong></p> <p>In fact, some of the style transfer results shown earlier are reminiscent of the results from <a href="https://en.wikipedia.org/wiki/DeepDream" rel="external nofollow noopener" target="_blank">DeepDream</a> and <a href="https://www.v7labs.com/blog/neural-style-transfer" rel="external nofollow noopener" target="_blank">Neural Style Transfer</a> work by Gatys et al. in their paper <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf" rel="external nofollow noopener" target="_blank">“Image Style Transfer Using Convolutional Neural Networks”</a> in 2016. The main caveat being that the results weren’t as impressive, and they used completely different architectures and training methods to achieve the results. Several other models like Google’s Gemini are multi-modal as well and support similar image generation capabilities. <a href="https://x.com/OriolVinyalsML/status/1899853815056085062" rel="external nofollow noopener" target="_blank">Here’s</a> the Gemini announcement for instance:</p> <div class="container"> <div class="row justify-content-center"> <div class="row-md-4"> <div class="jekyll-twitter-plugin"> <blockquote class="twitter-tweet"> <p lang="en" dir="ltr">Gemini 2.0 Flash debuts native image gen! Create contextually relevant images, edit conversationally, and generate long text in images. All totally optimized for chat iteration.<br><br>Try it in AI Studio or Gemini API. Blog: <a href="https://t.co/pkeRzaD8b5" rel="external nofollow noopener" target="_blank">https://t.co/pkeRzaD8b5</a> <a href="https://t.co/c7QUzNfC4k" rel="external nofollow noopener" target="_blank">pic.twitter.com/c7QUzNfC4k</a></p>— Oriol Vinyals (@OriolVinyalsML) <a href="https://twitter.com/OriolVinyalsML/status/1899853815056085062?ref_src=twsrc%5Etfw" rel="external nofollow noopener" target="_blank">March 12, 2025</a> </blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> </div> </div> </div> <p>However, none of them managed to capture the public imagination like this release from OpenAI. The main reason for the relative lack of fanfare regarding the Gemini announcement was two-fold:</p> <ul> <li>The Gemini releases were not product centric. The Gemini image generation model is only available as a preview model in Google AI Studio, and via Gemini API, and not in the main Gemini web interface. GPT-4o, on the other hand is available for users in the main ChatGPT website, immediately after the announcement. This is where network effects of over 400 million Weekly Active Users (WAUs) of ChatGPT come into play.</li> <li>The other harsh truth is that the image outputs of the Gemini model are simply not as good as the GPT-4o model. The results of both the Gemini model and the GPT-4o model on the same prompts are shown later <a href="#putting-openais-image-generation-to-the-test">here</a>.</li> </ul> <h3 id="putting-openais-image-generation-to-the-test">Putting OpenAI’s image generation to the test</h3> <div class="container"> <div class="row justify-content-center"> <div class="col-md-4"> <div style="position: relative;"> <img-comparison-slider hover="hover" value="70"> <figure slot="first"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post3/shashank-original-480.webp 480w,/assets/img/post3/shashank-original-800.webp 800w,/assets/img/post3/shashank-original-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/post3/shashank-original.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure slot="second"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post3/shashank-ghibli-openai-480.webp 480w,/assets/img/post3/shashank-ghibli-openai-800.webp 800w,/assets/img/post3/shashank-ghibli-openai-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/post3/shashank-ghibli-openai.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </img-comparison-slider> <div class="caption-overlay">OpenAI GPT-4o Ghibli Style</div> </div> </div> <div class="col-md-4"> <div style="position: relative;"> <img-comparison-slider hover="hover" value="70"> <figure slot="first"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post3/shashank-original-480.webp 480w,/assets/img/post3/shashank-original-800.webp 800w,/assets/img/post3/shashank-original-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/post3/shashank-original.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure slot="second"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post3/shashank-ghibli-grok-480.webp 480w,/assets/img/post3/shashank-ghibli-grok-800.webp 800w,/assets/img/post3/shashank-ghibli-grok-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/post3/shashank-ghibli-grok.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </img-comparison-slider> <div class="caption-overlay">Grok 3 Ghibli Style</div> </div> </div> <div class="col-md-4"> <div style="position: relative;"> <img-comparison-slider hover="hover" value="70"> <figure slot="first"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post3/shashank-original-480.webp 480w,/assets/img/post3/shashank-original-800.webp 800w,/assets/img/post3/shashank-original-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/post3/shashank-original.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure slot="second"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post3/shashank-ghibli-gemini-480.webp 480w,/assets/img/post3/shashank-ghibli-gemini-800.webp 800w,/assets/img/post3/shashank-ghibli-gemini-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/post3/shashank-ghibli-gemini.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </img-comparison-slider> <div class="caption-overlay">Google Gemini Ghibli Style</div> </div> </div> </div> </div> <p><br></p> <p><strong>Are the results worth the hype?</strong></p> <p>I compare the quality of the various multimodal models such as OpenAI’s <code class="language-plaintext highlighter-rouge">GPT-4o</code>, XAI’s <code class="language-plaintext highlighter-rouge">Grok 3</code>, and Google’s <code class="language-plaintext highlighter-rouge">Gemini 2.0 Pro Experimental</code> by testing them on the same input image and prompt. Following up on the Studio Ghibli trend, the prompt was “Create image - Convert this to studio ghibli style”. The final image generated by each model is shown here for comparison. We can see that both <code class="language-plaintext highlighter-rouge">GPT-4o</code> and <code class="language-plaintext highlighter-rouge">Grok 3</code> generate pretty reasonable and visually appealing results. The Gemini model, on the other hand, does not capture the intent of the prompt as well as the other two models. When running further tests on images with more people, <code class="language-plaintext highlighter-rouge">GPT-4o</code> edges out <code class="language-plaintext highlighter-rouge">Grok 3</code> in a bunch of cases, but both these models are roughly on par with each other.</p> <p><code class="language-plaintext highlighter-rouge">GPT-4o</code> is also great at generating images in the style of other artists, animation studios, and art movements. For instance, it is capable of generating art in the form of Ukiyo-e, Art Deco, Pixar, Minecraft, Ufotable, and more. It is also capable of arbitrarily changing the background or context of an image, as shown in the comparison slider on the bottom right. These models are only going to get better with time, and it does raise some concerns about what is real and what is not. But matter of fact is that this technology is here to stay, and there is no choice but to adapt to it.</p> <div class="container"> <div class="row"> <div class="col-md-5"> <div class="col-12"> <swiper-container keyboard="true" navigation="true" pagination="true" pagination-clickable="true" pagination-dynamic-bullets="true" rewind="true"> <swiper-slide> <div class="position-relative"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post3/shashank-ufotable-openai-480.webp 480w,/assets/img/post3/shashank-ufotable-openai-800.webp 800w,/assets/img/post3/shashank-ufotable-openai-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/post3/shashank-ufotable-openai.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption-overlay">Ufotable Style Portrait</div> </div> </swiper-slide> <swiper-slide> <div class="position-relative"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post3/shashank-starry-night-openai-480.webp 480w,/assets/img/post3/shashank-starry-night-openai-800.webp 800w,/assets/img/post3/shashank-starry-night-openai-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/post3/shashank-starry-night-openai.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption-overlay">Starry Night Style</div> </div> </swiper-slide> <swiper-slide> <div class="position-relative"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post3/shashank-minecraft-openai-480.webp 480w,/assets/img/post3/shashank-minecraft-openai-800.webp 800w,/assets/img/post3/shashank-minecraft-openai-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/post3/shashank-minecraft-openai.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption-overlay">Minecraft Style</div> </div> </swiper-slide> </swiper-container> </div> </div> <div class="col-md-3"> <swiper-container keyboard="true" navigation="true" pagination="true" pagination-clickable="true" rewind="true" style="height: 100%;"> <swiper-slide> <div class="position-relative"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post3/shashank-manga-openai-480.webp 480w,/assets/img/post3/shashank-manga-openai-800.webp 800w,/assets/img/post3/shashank-manga-openai-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/post3/shashank-manga-openai.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption-overlay">Manga Style</div> </div> </swiper-slide> <swiper-slide> <div class="position-relative"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post3/shashank-art-deco-openai-480.webp 480w,/assets/img/post3/shashank-art-deco-openai-800.webp 800w,/assets/img/post3/shashank-art-deco-openai-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/post3/shashank-art-deco-openai.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption-overlay">Art Deco Style</div> </div> </swiper-slide> <swiper-slide> <div class="position-relative"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post3/shashank-ukiyo-e-openai-480.webp 480w,/assets/img/post3/shashank-ukiyo-e-openai-800.webp 800w,/assets/img/post3/shashank-ukiyo-e-openai-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/post3/shashank-ukiyo-e-openai.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption-overlay">Ukiyo-e Style</div> </div> </swiper-slide> <swiper-slide> <div class="position-relative"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post3/shashank-pixar-openai-480.webp 480w,/assets/img/post3/shashank-pixar-openai-800.webp 800w,/assets/img/post3/shashank-pixar-openai-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/post3/shashank-pixar-openai.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption-overlay">Pixar Style</div> </div> </swiper-slide> </swiper-container> </div> <div class="col-md-4"> <div style="position: relative;"> <img-comparison-slider hover="hover" value="70"> <figure slot="first"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post3/crater-lake-480.webp 480w,/assets/img/post3/crater-lake-800.webp 800w,/assets/img/post3/crater-lake-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/post3/crater-lake.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure slot="second"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post3/sequoia-480.webp 480w,/assets/img/post3/sequoia-800.webp 800w,/assets/img/post3/sequoia-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/post3/sequoia.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </img-comparison-slider> <div class="caption-overlay">Background Replacement</div> </div> </div> </div> </div> <p><br></p> <style>.position-relative{position:relative;width:100%}.caption-overlay{position:absolute;bottom:0;left:0;right:0;background-color:rgba(0,0,0,0.7);color:white;padding:8px 15px;text-align:center;border-bottom-left-radius:4px;border-bottom-right-radius:4px}</style> <h4 id="is-it-a-fad-or-a-big-deal">Is it a fad or a big deal?</h4> <p>The Studio Ghibli trend went viral on social media, and it was akin to the original ChatGPT moment, albeit on a much smaller scale. Just like any viral trend on social media, this too will eventually fade away. However, unlike other highly hyped trends in the Generative AI space, such as video generation models like <a href="https://pika.art/login" rel="external nofollow noopener" target="_blank">Pika AI</a>, Autonomous agents like <a href="https://devin.ai/" rel="external nofollow noopener" target="_blank">Devin AI</a>, <a href="https://manus.im/" rel="external nofollow noopener" target="_blank">Manus</a>, and GPT stores like <a href="https://openai.com/index/introducing-gpts/" rel="external nofollow noopener" target="_blank">Custom GPTs</a>, this trend is most likely more secular. According to Sam Altman, Native image generation has already resulted in OpenAI gaining over 1 million users in an <a href="https://x.com/sama/status/1906771292390666325" rel="external nofollow noopener" target="_blank">hour</a> and <a href="https://x.com/sama/status/1905296867145154688" rel="external nofollow noopener" target="_blank">overloaded their servers</a> resulting in them adding temporary rate limits.</p> <h3 id="real-world-use-cases-of-this-technology">Real world use-cases of this technology</h3> <p>Right now, the biggest use case for these models seems to be simply generating cute/fun images in different styles. Even this simple use case will result in the release of new consumer-facing applications and tools. However, this technology also opens up several other interesting possibilities, especially in the virtual and augmented reality space, as well as in the realm of user interfaces.</p> <h4 id="generative-reality"><strong>Generative Reality</strong></h4> <p>As the ability of these models improve over time, and the latency of generating these images improves, there is a future where these models can be used to generate images in real-time, and even alter the notion of reality around us. This is what I call “Generative Reality”, which is the topic of the blog post. Generative Reality is not exactly a concept that’s defined in the literature, and it’s been discussed in various forms in the past in science fiction shows like <a href="https://www.imdb.com/title/tt11680642/" rel="external nofollow noopener" target="_blank">Pantheon</a> (must watch for science fiction fans), books like <a href="https://en.wikipedia.org/wiki/Ready_Player_One" rel="external nofollow noopener" target="_blank">Ready Player One</a>, and <a href="https://en.wikipedia.org/wiki/Snow_Crash" rel="external nofollow noopener" target="_blank">Snow Crash</a>. Most of these books/shows however focus on the concept of Virtual Reality (VR) and “Generative Reality” is slightly different. It is similar to Augmented Reality except that instead of overlaying information on top of the real world, “Generative Reality” refers to the process of altering the way the world looks while keeping it grounded in reality. An example of replacing the “real world” with a Studio Ghibli style world using a headset is shown below.</p> <div class="row justify-content-center w-100"> <div class="col-md-6"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post3/generative-reality-ghibli-480.webp 480w,/assets/img/post3/generative-reality-ghibli-800.webp 800w,/assets/img/post3/generative-reality-ghibli-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/post3/generative-reality-ghibli.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figcaption class="figure-caption text-center mt-2"> Generative Reality - Replacing the real world with a Studio Ghibli style world </figcaption> </div> <div class="col-md-6"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post3/generative-reality-daylight-savings-480.webp 480w,/assets/img/post3/generative-reality-daylight-savings-800.webp 800w,/assets/img/post3/generative-reality-daylight-savings-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/post3/generative-reality-daylight-savings.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figcaption class="figure-caption text-center mt-2"> Generative Reality - Replacing the real world with a Studio Ghibli style world </figcaption> </div> </div> <p><br></p> <h5 id="dystopian-nightmare-a-solution-to-switching-the-clocks">Dystopian Nightmare: A solution to switching the clocks</h5> <p>The debate around Universal standard time and Daylight Savings Time resurfaces during the first week of November and March every year, when the clocks are set back or forward. Proponents of Universal Time argue it aligns with natural circadian rhythms, while Daylight Savings advocates favor extended evening daylight for leisure. Here’s a tongue-in-cheek (and dystopian) idea: instead of changing clocks, Generative Reality could adjust how the world looks based on personal preference. Prefer Daylight Savings? Your headset could render the world as if it’s an hour ahead.</p> <h4 id="generative-ui"><strong>Generative UI</strong></h4> <p>A much more practical use case for native image generation is <a href="https://uxdesign.cc/an-introduction-to-generative-uis-01dcf6bca808" rel="external nofollow noopener" target="_blank">“Generative UI”</a> — using generative models to create adaptive, context-aware user interfaces.</p> <p>Generative UI can be used to create personalized user interfaces that adapt to the user’s needs and preferences in real-time. For example, if a user is booking a business trip via Airbnb, the generative UI can automatically adjust the layout, to focus more on important information like commute time to the downtown, or the speed of Wi-Fi at the hotel. On the other hand, family vacation booking might look more like existing Airbnb interfaces, with greater emphasis on family-friendly amenities, safety features, and cost comparisons as shown below. Unlike “Generative Reality”, this is not a pipe dream, and is already supported to a certain extent by tools like Vercel’s <a href="https://sdk.vercel.ai/docs/ai-sdk-ui/generative-user-interfaces" rel="external nofollow noopener" target="_blank">AI SDK</a>.</p> <div class="container"> <div class="row justify-content-center"> <div class="col-md-10"> <div style="position: relative;"> <img-comparison-slider hover="hover" value="60"> <figure slot="first"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post3/airbnb-family-480.webp 480w,/assets/img/post3/airbnb-family-800.webp 800w,/assets/img/post3/airbnb-family-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/post3/airbnb-family.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure slot="second"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post3/airbnb-business-480.webp 480w,/assets/img/post3/airbnb-business-800.webp 800w,/assets/img/post3/airbnb-business-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/post3/airbnb-business.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </img-comparison-slider> <div class="caption-overlay">Generative UI for two different usecases: Family Travel &amp; Business Travel.</div> </div> </div> </div> </div> <p><br></p> <h3 id="whats-next"><strong>What’s Next?</strong></h3> <p>In terms of immediate industry headwinds, companies like Adobe, Figma, and stock photo platforms like Shutterstock and Unsplash are likely to be affected the most by the rise of native image generation models.</p> <ul> <li> <p><strong>Adobe and Figma</strong> are obviously affected the most, as their tools are heavily used for graphic design, UI/UX design, mockups, and image editing. Even if the current models cannot fully replace the need for professional design tools completely, they will only get better at instruction following over time, and will replace the need for these tools amongst the more casual users and hobbyists.</p> </li> <li> <p>The other companies that face significant negative headwinds are stock photo hosting and sharing platforms like <strong>Shutterstock and Unsplash</strong>. The impact on these companies is not as clear-cut, since other Image generation models like Midjourney and Stable Diffusion have been around for a while, and have not significantly disrupted this market yet.</p> </li> <li> <p>There’s also potentially positive headwinds for <strong>AR/VR</strong> companies like Meta, Apple, Samsung, and more, since these models can help with creating newer User interfaces that are likely to make the AR/VR experience more immersive and personalized.</p> </li> <li> <p><strong>Social media companies</strong> like Meta, Snapchat, and TikTok could also integrate these tools into their platforms to allow users to add more interactive filters to their stories and posts, increasing user engagement and retention.</p> </li> </ul> <p>As far as the core LLM technology is concerned, the logical next step would be to train these multi-modal models on even larger datasets, and even more modalities like video, and 3D models. Companies like <a href="https://worldlabs.ai/blog" rel="external nofollow noopener" target="_blank">World Labs</a> are already working on 3D world generation using LLMs / Foundation Models, and future frontier models like <code class="language-plaintext highlighter-rouge">Gemini-3.0</code>, <code class="language-plaintext highlighter-rouge">Grok 4</code>, <code class="language-plaintext highlighter-rouge">GPT-6</code> and more will likely have support for autoregressive video and 3D world generation as well. The priors for this happening are quite high, due to the existence of several large datasets of videos and 3D models available on the internet:</p> <ul> <li> <strong>Video datasets</strong>: All the content in YouTube, TikTok, movies, and more</li> <li> <strong>3D datasets</strong>: Data from varioues sources like Lidar scans from autonomous vehicles, 3D assets in game engines like Unreal Engine, Unity, and Blender, and spatial reconstruction data from NeRFs (Neural Radiance Fields) deployed in smartphone cameras and other devices.</li> </ul> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Shashank Srikanth. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Last updated: April 01, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?70d799092f862ad98c7876aa47712e20"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/img-comparison-slider@8.0.6/dist/index.min.js" integrity="sha256-EXHg3x1K4oIWdyohPeKX2ZS++Wxt/FRPH7Nl01nat1o=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/swiper@11.0.5/swiper-element-bundle.min.js" integrity="sha256-BPrwikijIybg9OQC5SYFFqhBjERYOn97tCureFgYH1E=" crossorigin="anonymous"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>