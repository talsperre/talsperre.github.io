<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> OpenAI Native Image Generation - Generative Reality, Generative UI, and more | Shashank Srikanth </title> <meta name="author" content="Shashank Srikanth"> <meta name="description" content="Generative Reality - The next step in native image generation"> <meta name="keywords" content="Metaflow, Shashank Srikanth, tech, blog, academic-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://talsperre.github.io/blog/2025/generated-reality/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/img-comparison-slider@8.0.6/dist/styles.min.css" integrity="sha256-3qTIuuUWIFnnU3LpQMjqiXc0p09rvd0dmj+WkpQXSR8=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/swiper@11.0.5/swiper-bundle.min.css" integrity="sha256-yUoNxsvX+Vo8Trj3lZ/Y5ZBf8HlBFsB6Xwm7rH75/9E=" crossorigin="anonymous"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Shashank</span> Srikanth </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">GitHub Profile </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">OpenAI Native Image Generation - Generative Reality, Generative UI, and more</h1> <p class="post-meta"> Created in February 16, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a> ¬† ¬∑ ¬† <a href="/blog/tag/llms"> <i class="fa-solid fa-hashtag fa-sm"></i> llms</a> ¬† <a href="/blog/tag/ar"> <i class="fa-solid fa-hashtag fa-sm"></i> ar</a> ¬† <a href="/blog/tag/openai"> <i class="fa-solid fa-hashtag fa-sm"></i> openai</a> ¬† ¬∑ ¬† <a href="/blog/category/tech"> <i class="fa-solid fa-tag fa-sm"></i> tech</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>A short post on native image generation in LLMs, OpenAI‚Äôs GPT-4o release and Studio Ghibli trend, and applications of native image generation like ‚ÄúGenerative UI‚Äù and ‚ÄúGenerative Reality‚Äù.</p> <h2 id="openai-native-image-generation"><strong>OpenAI Native Image Generation</strong></h2> <p>OpenAI <a href="https://openai.com/index/introducing-4o-image-generation/" rel="external nofollow noopener" target="_blank">announced</a> native image generation in <code class="language-plaintext highlighter-rouge">GPT-4o</code> on March 25th, 2025. Immediately after the announcement, Machine Learning <a href="https://grok.com/share/bGVnYWN5_9177404c-2e12-4246-abc4-5b422ec48a9a" rel="external nofollow noopener" target="_blank">TPOT</a> (This Part of Twitter) was flooded with tweets the model‚Äôs ability to generate a version of the provided image in the style of <a href="https://en.wikipedia.org/wiki/Studio_Ghibli" rel="external nofollow noopener" target="_blank">Studio Ghibli</a>, an animation style initially popularized by Hayao Miyazaki and Isao Takahata. The ‚Äúoffending‚Äù tweet by <a href="https://x.com/GrantSlatton/status/1904631016356274286" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">@GrantSlatton</code></a> is provided below (left). People were quick to jump on the bandwagon, with users posting ‚ÄúGhiblifying‚Äù images of their friends, families, and iconic pictures from both pop/meme culture.</p> <div class="container"> <div class="row justify-content-center"> <div class="col-md-6"> <div class="jekyll-twitter-plugin"> <blockquote class="twitter-tweet"> <p lang="en" dir="ltr">tremendous alpha right now in sending your wife photos of yall converted to studio ghibli anime <a href="https://t.co/FROszdFSfN" rel="external nofollow noopener" target="_blank">pic.twitter.com/FROszdFSfN</a></p>‚Äî Grant Slatton (@GrantSlatton) <a href="https://twitter.com/GrantSlatton/status/1904631016356274286?ref_src=twsrc%5Etfw" rel="external nofollow noopener" target="_blank">March 25, 2025</a> </blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> </div> <div class="col-md-6"> <div class="jekyll-twitter-plugin"> <blockquote class="twitter-tweet"> <p lang="en" dir="ltr">Iconic movie scenes, Studio Ghibli style. This is so much fun! <a href="https://t.co/LwjkNjcEV9" rel="external nofollow noopener" target="_blank">pic.twitter.com/LwjkNjcEV9</a></p>‚Äî Mufaddal Durbar (@MDurbar) <a href="https://twitter.com/MDurbar/status/1904872441899339963?ref_src=twsrc%5Etfw" rel="external nofollow noopener" target="_blank">March 26, 2025</a> </blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> </div> </div> </div> <p>Even though, generating ghiblified versions of images is fun, these native image generation models are capable of much more than that. They can potentially remove watermarks from images, add/remove subjects from images, help with interior design and more. What‚Äôs interesting is that despite all the hype about AI art and AI generated images, the most popular use case right now seems to be modifying existing images of friends, family, and pets in various styles. Imitation is the sincerest form of flattery, and the fact that people are using these models to add filters on top of their own photos is extremely revealing of two things:</p> <ul> <li>True creativity is still exceptionally rare and very valuable,</li> <li>Providing some notion of control over the output space rather than a blank canvas is more appealing to the average user.</li> </ul> <p><em>This newfound capability raises a natural question: what exactly is native image generation, and how does it differ from earlier approaches? Let‚Äôs dive into the technical details.</em></p> <h3 id="what-is-native-image-generation">What is Native Image Generation?</h3> <p>Earlier image generation models like <code class="language-plaintext highlighter-rouge">DALL-E</code>, <code class="language-plaintext highlighter-rouge">CLIP</code>, <code class="language-plaintext highlighter-rouge">Imagen</code>, and more relied on diffusion models, Vision Transformers, or Generative Adversarial Networks (does anyone still remember GANs?). In contrast, newer models like <code class="language-plaintext highlighter-rouge">GPT-4o</code>, <code class="language-plaintext highlighter-rouge">Grok 3</code>, and <code class="language-plaintext highlighter-rouge">gemini-2.0-flash-exp-image-generation</code> (yes, that‚Äôs really the name ü§¶‚ÄçÔ∏è) are truly multimodal. These models can generate images and audio in an autoregressive manner, much like they generate text. The GIF below illustrates how autoregressive text generation works in an earlier model like <code class="language-plaintext highlighter-rouge">GPT-2</code>. Autoregressive in the context of an LLM means that in order to generate the next token, the model conditions its prediction on the provided context (prompt and system prompt) and the previously generated tokens. For instance if the system prompt is ‚ÄúAnswer the users question in a single sentence‚Äù and the prompt is ‚ÄúRecite the first law of robotics‚Äù, the following steps are taken:</p> <ul> <li>The model (GPT-2) encodes the system prompt and the user prompt and updates the state of the Key, Query, and Value vectors (attention mechanism) in the transformer model.</li> <li>The model generates the first token <code class="language-plaintext highlighter-rouge">"A"</code> based on this internal state.</li> <li>The model modifies the internal state with the output <code class="language-plaintext highlighter-rouge">"A"</code> and then uses this modified state to predict the next token <code class="language-plaintext highlighter-rouge">"robot"</code>.</li> <li>The model continues this process until the end of the sequence is reached, which is either a maximum length or a special token like <code class="language-plaintext highlighter-rouge">&lt;|endoftext|&gt;</code>.</li> </ul> <p>As the other image on the right shows, multimodal models like <code class="language-plaintext highlighter-rouge">GPT-4o</code> treat text, image pixels, and audio waveforms as different tokens, and jointly train across all three modalities. During generation, predict a pixel, a word, or a waveform at a time. In a very naive sense, the model can be thought of as generating pixels from left-to-right, and top-to-bottom.</p> <div class="row justify-content-center w-100"> <div class="col-md-8"> <figure class="figure"> <img src="/assets/img/post3/gpt-2-autoregression-2.gif" alt="Autoregressive generation" class="img-fluid rounded z-depth-1" onclick="this.classList.toggle('zoomed')"> <figcaption class="figure-caption text-center mt-2">Autoregressive text generation process in GPT-2 <a href="https://jalammar.github.io/illustrated-gpt2/" rel="external nofollow noopener" target="_blank">[*]</a></figcaption> </figure> </div> <div class="col-md-4"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post3/multimodal-llms-480.webp 480w,/assets/img/post3/multimodal-llms-800.webp 800w,/assets/img/post3/multimodal-llms-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/post3/multimodal-llms.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figcaption class="figure-caption text-center mt-2">Multimodal Large Language Model architecture</figcaption> </div> </div> <p><br></p> <p>The main advantage of this approach is efficiency: we no longer need specialized systems for different modalities, streamlining both training and inference. Additionally, these models leverage cross-modal relationships, enhancing their contextual understanding of scenes.</p> <h3 id="how-does-it-really-work">How Does It Really Work?</h3> <p>Multimodal LLMs fall into two main categories:</p> <ul> <li>Models that process multiple input modalities (e.g., images, audio, text) but only generate text as output, such as <a href="https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/" rel="external nofollow noopener" target="_blank">LLaMA 3.2</a>.</li> <li>Models that can also generate images or audio as output, like <code class="language-plaintext highlighter-rouge">GPT-4o</code> and <code class="language-plaintext highlighter-rouge">Grok 3</code>.</li> </ul> <h4 id="deeper-dive-into-multi-modal-models">Deeper dive into multi-modal models</h4> <div class="row justify-content-center w-100"> <div class="col-md-12"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post3/multimodal-llm-input-480.webp 480w,/assets/img/post3/multimodal-llm-input-800.webp 800w,/assets/img/post3/multimodal-llm-input-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/post3/multimodal-llm-input.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figcaption class="figure-caption text-center mt-2">Processing image using VIT for multi-modal LLMs <a href="https://sebastianraschka.com/blog/2024/understanding-multimodal-llms.html" rel="external nofollow noopener" target="_blank">[*] </a></figcaption> </div> </div> <p><br></p> <p>The mechanism of processing multimodal inputs is roughly similar in both these types of models but the output generation is different. The image below shows how a multi-modal LLM can process an image as well as text together to generate a response.</p> <ul> <li>These multi-modal models typically process an image into a smaller chunks of size <code class="language-plaintext highlighter-rouge">16 x 16</code> or <code class="language-plaintext highlighter-rouge">32 x 32</code> pixels, in a left to right, top to bottom manner.</li> <li>These chunks are fed in a sequential manner to another model, typically a Vision Transformer (ViT), which processes these image chunks and generates a representation for each chunk. These intermediate representations are then fed into a linear projection layer, which resizes the image representations to the same dimensionality as the input text embeddings, and also ensures that the generated image embeddings are in the same ‚Äúlatent space‚Äù as the text embeddings.</li> <li>This alignment is done by training the model, specifically the projection layers on a large dataset of text-image pairs, after the base LLM has finished training.</li> </ul> <p>Models from different research groups use varying approaches for training, especially regarding which layers to freeze and which to update, but it common to only update the linear projection layer and image encoder during training. For instance, see this snippet from the LLama 3.2 blog post:</p> <blockquote> <p>To add image input support, we trained a set of adapter weights that integrate the pre-trained image encoder into the pre-trained language model. The adapter consists of a series of cross-attention layers that feed image encoder representations into the language model. We trained the adapter on text-image pairs to align the image representations with the language representations. During adapter training, we also updated the parameters of the image encoder, but intentionally did not update the language-model parameters. By doing that, we keep all the text-only capabilities intact, providing developers a drop-in replacement for Llama 3.1 models.</p> </blockquote> <p>This <a href="https://sebastianraschka.com/blog/2024/understanding-multimodal-llms.html" rel="external nofollow noopener" target="_blank">blog post</a> by Sebastian Raschka provides a more technical deep dive into how multi-modal models are trained, and the state-of-the-art models in this space as of late <code class="language-plaintext highlighter-rouge">2024</code>. However, the blog post does not cover the generation/decoding process for multimodal models like <code class="language-plaintext highlighter-rouge">GPT-4o</code> and <code class="language-plaintext highlighter-rouge">Grok 3</code>, which not only process multimodal inputs, but are also capable of generating images in an autoregressive manner. The concept of generating images in an autoregressive manner is not new, and has been explored as far back as the 2016s in papers like <a href="https://arxiv.org/abs/1601.06759" rel="external nofollow noopener" target="_blank">PixelRNN</a>. PixelRNN simply used an RNN to predict the next pixel in an image, given the previous pixels.</p> <p>However, there have been several improvements to such models in recent years, with most of the improvements coming in the way that</p> <ol> <li>Images are encoded (CLIP/VQ-VAE) and,</li> <li>Modifying the process of autoregressive generation itself from sequential visual token generation in a raster-scan order, to more complex generation processes like Visual AutoRegressive modeling (VAR), which is autoregressive generation of images as coarse-to-fine ‚Äúnext-scale prediction‚Äù or ‚Äúnext-resolution prediction‚Äù.</li> </ol> <p>Visual AutoRegressive modeling was introduced in the 2024 NeurIPS Best Paper award-winning paper <a href="https://arxiv.org/pdf/2404.02905" rel="external nofollow noopener" target="_blank">‚ÄúVisual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction‚Äù</a>.</p> <ul> <li>The model first generates a low-resolution image starting from a \(1 \times 1\) token map, and then progressively increases the resolution by making the transformer based model to predict the higher resolution map.</li> <li>Each higher resolution map is conditioned on the previous lower resolution map, thus making the generation process autoregressive.</li> <li>Importantly, the authors show empirical validation of te scaling laws and zero-shot generalization potential of the VAR model, which is markedly similar to those of other LLMs.</li> </ul> <div class="fun-fact-container"> <div class="fun-fact-header"> <h4>Drama behind this Best Paper</h4> <span class="fun-fact-toggle">+</span> </div> <div class="fun-fact-content"> <p>The lead author of this paper, a ByteDance intern at that time, was accused of <a href="https://www.wired.com/story/bytedance-intern-best-paper-neurips/" rel="external nofollow noopener" target="_blank">sabotaging</a> other researchers training jobs within ByteDance by modifying the PyTorch source code. More details are available in this <a href="https://www.reddit.com/r/MachineLearning/comments/1hctf36/d_the_winner_of_the_neurips_2024_best_paper_award/" rel="external nofollow noopener" target="_blank">reddit</a> post but I haven't verified the veracity of these claims.</p> </div> </div> <style>.fun-fact-container{margin:20px 0;border-radius:8px;overflow:hidden;box-shadow:0 2px 5px rgba(0,0,0,0.1);background-color:#f8f9fa}.fun-fact-header{display:flex;justify-content:space-between;align-items:center;padding:12px 15px;background-color:#e9ecef;cursor:pointer;transition:background-color .3s ease}.fun-fact-header:hover{background-color:#dee2e6}.fun-fact-header h4{margin:0;font-size:1.1rem;color:#495057}.fun-fact-toggle{font-size:1.5rem;font-weight:bold;transition:transform .3s ease}.fun-fact-container.active .fun-fact-toggle{transform:rotate(45deg)}.fun-fact-content{max-height:0;overflow:hidden;padding:0 15px;transition:max-height .3s ease,padding .3s ease}.fun-fact-container.active .fun-fact-content{max-height:500px;padding:15px}</style> <script>
    document.addEventListener('DOMContentLoaded', function() {
      const funFactContainers = document.querySelectorAll('.fun-fact-container');

      funFactContainers.forEach(container => {
        const header = container.querySelector('.fun-fact-header');

        header.addEventListener('click', function() {
          container.classList.toggle('active');
        });
      });
    });
  </script> <div class="row justify-content-center w-100"> <div class="col-md-12"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post3/var-auto-regressive-models-480.webp 480w,/assets/img/post3/var-auto-regressive-models-800.webp 800w,/assets/img/post3/var-auto-regressive-models-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/post3/var-auto-regressive-models.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figcaption class="figure-caption text-center mt-2">A Visual AutoRegressive model generates images in a coarse-to-fine manner <a href="https://arxiv.org/pdf/2404.02905" rel="external nofollow noopener" target="_blank">[*]</a></figcaption> </div> </div> <p><br></p> <p>The diagram above shows the process of VAR generation as described in the paper. This is not the only way to generate images in an autoregressive manner, and it isn‚Äôt clear if <code class="language-plaintext highlighter-rouge">GPT-4o</code> or other native image generation models like <code class="language-plaintext highlighter-rouge">Gemini 2.0 Pro Experimental</code> and <code class="language-plaintext highlighter-rouge">Grok 3</code> use similar techniques. There have been some speculations that the GPT-4o model performs autoregressive generation of images in a raster-scan order (left to right, top to bottom), with generation happening at all scales simultaneously. Another speculation is that there are two separate models, <code class="language-plaintext highlighter-rouge">GPT-4o</code> generates tokens in the image latent space, and a separate diffusion based decoder generates the actual image - see tweets below. Given the artifacts being generated in the ChatGPT UI like top-down generation, blurry intermediates, etc., I believe the latter speculation is more likely.</p> <div class="container"> <div class="row justify-content-center"> <div class="col-md-6"> <div class="jekyll-twitter-plugin"> <blockquote class="twitter-tweet"> <p lang="en" dir="ltr">Okay my working hypothesis for 4o image generation is that it is jointly performing autoregressive inference (raster scanline order) on an image pyramid at all scales simultaneously. <a href="https://t.co/WMoidIuvN3" rel="external nofollow noopener" target="_blank">https://t.co/WMoidIuvN3</a></p>‚Äî Jon Barron (@jon_barron) <a href="https://twitter.com/jon_barron/status/1905143840572583974?ref_src=twsrc%5Etfw" rel="external nofollow noopener" target="_blank">March 27, 2025</a> </blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> </div> <div class="col-md-6"> <div class="jekyll-twitter-plugin"> <blockquote class="twitter-tweet"> <p lang="en" dir="ltr">How would gpt-4o image generation work? Speculation:<br><br>- gpt-4o generates visual tokens, and the diffusion decoder decodes them to pixel space. <br>- Not just diffusion but Rolling Diffusion-like group-wise diffusion decoder, top-&gt;bottom ordering. <a href="https://t.co/4uIv8m7aIq" rel="external nofollow noopener" target="_blank">pic.twitter.com/4uIv8m7aIq</a></p>‚Äî Sangyun Lee (@sang_yun_lee) <a href="https://twitter.com/sang_yun_lee/status/1905411685499691416?ref_src=twsrc%5Etfw" rel="external nofollow noopener" target="_blank">March 28, 2025</a> </blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> </div> </div> </div> <h3 id="is-any-of-this-new"><strong>Is any of this new?</strong></h3> <p><strong>The resounding answer is no.</strong></p> <p>In fact, some of the style transfer results shown earlier are reminiscent of the results from <a href="https://en.wikipedia.org/wiki/DeepDream" rel="external nofollow noopener" target="_blank">DeepDream</a> and <a href="https://www.v7labs.com/blog/neural-style-transfer" rel="external nofollow noopener" target="_blank">Neural Style Transfer</a> work by Gatys et al. in their paper <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf" rel="external nofollow noopener" target="_blank">‚ÄúImage Style Transfer Using Convolutional Neural Networks‚Äù</a> in 2016. The main caveat being that the results weren‚Äôt as impressive, and they used completely different architectures and training methods to achieve the results. Several other models like Google‚Äôs Gemini are multi-modal as well and support similar image generation capabilities. <a href="https://x.com/OriolVinyalsML/status/1899853815056085062" rel="external nofollow noopener" target="_blank">Here‚Äôs</a> the Gemini announcement for instance:</p> <div class="container"> <div class="row justify-content-center"> <div class="row-md-4"> <div class="jekyll-twitter-plugin"> <blockquote class="twitter-tweet"> <p lang="en" dir="ltr">Gemini 2.0 Flash debuts native image gen! Create contextually relevant images, edit conversationally, and generate long text in images. All totally optimized for chat iteration.<br><br>Try it in AI Studio or Gemini API. Blog: <a href="https://t.co/pkeRzaD8b5" rel="external nofollow noopener" target="_blank">https://t.co/pkeRzaD8b5</a> <a href="https://t.co/c7QUzNfC4k" rel="external nofollow noopener" target="_blank">pic.twitter.com/c7QUzNfC4k</a></p>‚Äî Oriol Vinyals (@OriolVinyalsML) <a href="https://twitter.com/OriolVinyalsML/status/1899853815056085062?ref_src=twsrc%5Etfw" rel="external nofollow noopener" target="_blank">March 12, 2025</a> </blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> </div> </div> </div> <p>However, none of them managed to capture the public imagination like this release from OpenAI. The main reason for the relative lack of fanfare regarding the <code class="language-plaintext highlighter-rouge">Gemini</code> announcement was two-fold:</p> <ul> <li>The <code class="language-plaintext highlighter-rouge">Gemini</code> releases were not product centric. The <code class="language-plaintext highlighter-rouge">Gemini</code> image generation model is only available as a preview model in Google AI Studio, and via <code class="language-plaintext highlighter-rouge">Gemini</code> API, and not in the main Gemini web interface. <code class="language-plaintext highlighter-rouge">GPT-4o</code>, on the other hand is available for users in the main ChatGPT website, immediately after the announcement. This is where network effects of over 400 million Weekly Active Users (WAUs) of ChatGPT come into play.</li> <li>The other harsh truth is that the image outputs of the <code class="language-plaintext highlighter-rouge">Gemini</code> model are simply not as good as the <code class="language-plaintext highlighter-rouge">GPT-4o</code> model. The results of both the Gemini model and the <code class="language-plaintext highlighter-rouge">GPT-4o</code> model on the same prompts are shown later <a href="#putting-openais-image-generation-to-the-test">here</a>.</li> </ul> <h3 id="putting-openais-image-generation-to-the-test">Putting OpenAI‚Äôs image generation to the test</h3> <div class="container"> <div class="row justify-content-center"> <div class="col-md-4"> <div style="position: relative;"> <img-comparison-slider hover="hover" value="70"> <figure slot="first"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post3/shashank-original-480.webp 480w,/assets/img/post3/shashank-original-800.webp 800w,/assets/img/post3/shashank-original-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/post3/shashank-original.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure slot="second"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post3/shashank-ghibli-openai-480.webp 480w,/assets/img/post3/shashank-ghibli-openai-800.webp 800w,/assets/img/post3/shashank-ghibli-openai-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/post3/shashank-ghibli-openai.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </img-comparison-slider> <div class="caption-overlay">OpenAI GPT-4o Ghibli Style</div> </div> </div> <div class="col-md-4"> <div style="position: relative;"> <img-comparison-slider hover="hover" value="70"> <figure slot="first"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post3/shashank-original-480.webp 480w,/assets/img/post3/shashank-original-800.webp 800w,/assets/img/post3/shashank-original-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/post3/shashank-original.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure slot="second"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post3/shashank-ghibli-grok-480.webp 480w,/assets/img/post3/shashank-ghibli-grok-800.webp 800w,/assets/img/post3/shashank-ghibli-grok-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/post3/shashank-ghibli-grok.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </img-comparison-slider> <div class="caption-overlay">Grok 3 Ghibli Style</div> </div> </div> <div class="col-md-4"> <div style="position: relative;"> <img-comparison-slider hover="hover" value="70"> <figure slot="first"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post3/shashank-original-480.webp 480w,/assets/img/post3/shashank-original-800.webp 800w,/assets/img/post3/shashank-original-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/post3/shashank-original.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure slot="second"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post3/shashank-ghibli-gemini-480.webp 480w,/assets/img/post3/shashank-ghibli-gemini-800.webp 800w,/assets/img/post3/shashank-ghibli-gemini-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/post3/shashank-ghibli-gemini.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </img-comparison-slider> <div class="caption-overlay">Google Gemini Ghibli Style</div> </div> </div> </div> </div> <p><br></p> <p><strong>Are the results worth the hype?</strong></p> <p>I compare the quality of the various multimodal models such as OpenAI‚Äôs <code class="language-plaintext highlighter-rouge">GPT-4o</code>, XAI‚Äôs <code class="language-plaintext highlighter-rouge">Grok 3</code>, and Google‚Äôs <code class="language-plaintext highlighter-rouge">Gemini 2.0 Pro Experimental</code> by testing them on the same input image and prompt. Following up on the Studio Ghibli trend, the prompt was ‚ÄúCreate image - convert this to Studio Ghibli style‚Äù. The final image generated by each model is shown here for comparison. We can see that both <code class="language-plaintext highlighter-rouge">GPT-4o</code> and <code class="language-plaintext highlighter-rouge">Grok 3</code> generate pretty reasonable and visually appealing results. The Gemini model, on the other hand, does not capture the intent of the prompt as well as the other two models. When running further tests on images with more people, <code class="language-plaintext highlighter-rouge">GPT-4o</code> edges out <code class="language-plaintext highlighter-rouge">Grok 3</code> in a bunch of cases, but both these models are roughly on par with each other.</p> <p><code class="language-plaintext highlighter-rouge">GPT-4o</code> is also great at generating images in the style of other artists, animation studios, and art movements. For instance, it is capable of generating art in the form of Ukiyo-e, Art Deco, Pixar, Minecraft, Ufotable, and more. It is also capable of arbitrarily changing the background or context of an image, as shown in the comparison slider on the bottom right. In this example, I have a ‚Äúghiblified‚Äù image of myself and a group of friends in Crater Lake, and asked the model to change the background to Sequoia National Park by giving it the following prompt: ‚ÄúCreate image - convert this to Studio Ghibli but replace the background with Sequoia National Park‚Äù. The prompts for changing backgrounds doesn‚Äôt work well when operating on the original image directly. Modifying the prompt slighly to ‚ÄúCreate image - replace the background with Sequoia National Park‚Äù results in a spectacular failure. These models are only going to get better with time, and it does raise some concerns about what is real and what is not. But matter of fact is that this technology is here to stay, and there is no choice but to adapt to it.</p> <p><em>The failure output isn‚Äôt shown here, out of respect for my friends‚Äô privacy.</em></p> <div class="container"> <div class="row"> <div class="col-md-5"> <div class="col-12"> <swiper-container keyboard="true" navigation="true" pagination="true" pagination-clickable="true" pagination-dynamic-bullets="true" rewind="true"> <swiper-slide> <div class="position-relative"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post3/shashank-ufotable-openai-480.webp 480w,/assets/img/post3/shashank-ufotable-openai-800.webp 800w,/assets/img/post3/shashank-ufotable-openai-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/post3/shashank-ufotable-openai.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption-overlay">Ufotable Style Portrait</div> </div> </swiper-slide> <swiper-slide> <div class="position-relative"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post3/shashank-starry-night-openai-480.webp 480w,/assets/img/post3/shashank-starry-night-openai-800.webp 800w,/assets/img/post3/shashank-starry-night-openai-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/post3/shashank-starry-night-openai.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption-overlay">Starry Night Style</div> </div> </swiper-slide> <swiper-slide> <div class="position-relative"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post3/shashank-minecraft-openai-480.webp 480w,/assets/img/post3/shashank-minecraft-openai-800.webp 800w,/assets/img/post3/shashank-minecraft-openai-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/post3/shashank-minecraft-openai.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption-overlay">Minecraft Style</div> </div> </swiper-slide> </swiper-container> </div> </div> <div class="col-md-3"> <swiper-container keyboard="true" navigation="true" pagination="true" pagination-clickable="true" rewind="true" style="height: 100%;"> <swiper-slide> <div class="position-relative"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post3/shashank-manga-openai-480.webp 480w,/assets/img/post3/shashank-manga-openai-800.webp 800w,/assets/img/post3/shashank-manga-openai-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/post3/shashank-manga-openai.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption-overlay">Manga Style</div> </div> </swiper-slide> <swiper-slide> <div class="position-relative"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post3/shashank-art-deco-openai-480.webp 480w,/assets/img/post3/shashank-art-deco-openai-800.webp 800w,/assets/img/post3/shashank-art-deco-openai-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/post3/shashank-art-deco-openai.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption-overlay">Art Deco Style</div> </div> </swiper-slide> <swiper-slide> <div class="position-relative"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post3/shashank-ukiyo-e-openai-480.webp 480w,/assets/img/post3/shashank-ukiyo-e-openai-800.webp 800w,/assets/img/post3/shashank-ukiyo-e-openai-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/post3/shashank-ukiyo-e-openai.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption-overlay">Ukiyo-e Style</div> </div> </swiper-slide> <swiper-slide> <div class="position-relative"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post3/shashank-pixar-openai-480.webp 480w,/assets/img/post3/shashank-pixar-openai-800.webp 800w,/assets/img/post3/shashank-pixar-openai-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/post3/shashank-pixar-openai.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption-overlay">Pixar Style</div> </div> </swiper-slide> </swiper-container> </div> <div class="col-md-4"> <div style="position: relative;"> <img-comparison-slider hover="hover" value="70"> <figure slot="first"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post3/crater-lake-480.webp 480w,/assets/img/post3/crater-lake-800.webp 800w,/assets/img/post3/crater-lake-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/post3/crater-lake.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure slot="second"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post3/sequoia-480.webp 480w,/assets/img/post3/sequoia-800.webp 800w,/assets/img/post3/sequoia-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/post3/sequoia.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </img-comparison-slider> <div class="caption-overlay">Background Replacement</div> </div> </div> </div> </div> <p><br></p> <style>.position-relative{position:relative;width:100%}.caption-overlay{position:absolute;bottom:0;left:0;right:0;background-color:rgba(0,0,0,0.7);color:white;padding:8px 15px;text-align:center;border-bottom-left-radius:4px;border-bottom-right-radius:4px}</style> <h4 id="is-it-a-fad-or-a-big-deal">Is it a fad or a big deal?</h4> <p>The Studio Ghibli trend went viral on social media, and it was akin to the original ChatGPT moment, albeit on a much smaller scale. Just like any viral trend on social media, this too will eventually fade away. However, unlike other highly hyped trends in the Generative AI space, such as video generation models like <a href="https://pika.art/login" rel="external nofollow noopener" target="_blank">Pika AI</a>, Autonomous agents like <a href="https://devin.ai/" rel="external nofollow noopener" target="_blank">Devin AI</a>, <a href="https://manus.im/" rel="external nofollow noopener" target="_blank">Manus</a> etc., this trend seems to be more secular. According to Sam Altman, Native image generation has already resulted in OpenAI gaining over <code class="language-plaintext highlighter-rouge">1</code> million users in an <a href="https://x.com/sama/status/1906771292390666325" rel="external nofollow noopener" target="_blank">hour</a> and <a href="https://x.com/sama/status/1905296867145154688" rel="external nofollow noopener" target="_blank">overloaded their servers</a> resulting in them imposing temporary rate limits on free as well as paid users.</p> <h3 id="real-world-use-cases-of-this-technology">Real world use-cases of this technology</h3> <p>Right now, the biggest use case for these models seems to be simply generating cute/fun images in different styles. And even this simple use case will result in the release of new consumer-facing applications and tools. However, this technology also opens up several other interesting possibilities, especially in the virtual and augmented reality space, as well as in the realm of user interfaces.</p> <h4 id="generative-reality"><strong>Generative Reality</strong></h4> <p>As the ability of these models improve over time, and the latency of generating these images improves, there is a future where these models can be used to generate images in real-time, and even alter the notion of reality around us. This is what I call ‚ÄúGenerative Reality‚Äù, a new made-up term. The term ‚ÄúGenerative Reality‚Äù (<code class="language-plaintext highlighter-rouge">GR</code>) hasn‚Äôt been widely explored in academic literature, but it has appeared in various forms throughout science fiction shows like <a href="https://www.imdb.com/title/tt11680642/" rel="external nofollow noopener" target="_blank">Pantheon</a> (must watch for science fiction fans), and books like <a href="https://en.wikipedia.org/wiki/Ready_Player_One" rel="external nofollow noopener" target="_blank">Ready Player One</a>, and <a href="https://en.wikipedia.org/wiki/Snow_Crash" rel="external nofollow noopener" target="_blank">Snow Crash</a>. Most of these books/shows, however, focus on the concept of Virtual Reality (VR) and ‚ÄúGenerative Reality‚Äù is slightly different. Unlike Augmented Reality which overlays information on top of the real world, <code class="language-plaintext highlighter-rouge">GR</code> alters the way we perceive the world by generating new images in real-time via style transfer while keeping the generated images grounded in reality. For example, a Apple Vision Pro like headset can potentially replace the ‚Äúreal world‚Äù with a Studio Ghibli style world, as shown below. The existence of native image generation models like <code class="language-plaintext highlighter-rouge">GPT-4o</code> and <code class="language-plaintext highlighter-rouge">Grok 3</code> make this a real possibility at some point in the future.</p> <div class="row justify-content-center w-100"> <div class="col-md-6"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post3/generative-reality-ghibli-480.webp 480w,/assets/img/post3/generative-reality-ghibli-800.webp 800w,/assets/img/post3/generative-reality-ghibli-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/post3/generative-reality-ghibli.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figcaption class="figure-caption text-center mt-2"> Generative Reality - Replacing the real world with a Studio Ghibli style world </figcaption> </div> <div class="col-md-6"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post3/generative-reality-daylight-savings-480.webp 480w,/assets/img/post3/generative-reality-daylight-savings-800.webp 800w,/assets/img/post3/generative-reality-daylight-savings-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/post3/generative-reality-daylight-savings.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figcaption class="figure-caption text-center mt-2"> Generative Reality - Replacing the real world with a Studio Ghibli style world </figcaption> </div> </div> <p><br></p> <p>This <a href="https://www.bbc.com/news/world-europe-50571010" rel="external nofollow noopener" target="_blank">article</a> goes over a more dystopian example of using VR headsets to show a ‚Äúunique summer field simulation program‚Äù to Dairy cows to improve yields. A slightly less dystopian (still dystopian) example is to use <code class="language-plaintext highlighter-rouge">GR</code> to ‚Äúsolve‚Äù the Daylight Savings Time problem.</p> <h5 id="dystopian-nightmare-a-solution-to-switching-the-clocks">Dystopian Nightmare: A solution to switching the clocks</h5> <p>The debate around Universal standard time and Daylight Savings Time resurfaces during the first week of November and March every year, when the clocks are set back or forward. Proponents of Universal Time argue it aligns with natural circadian rhythms, while Daylight Savings advocates favor extended evening daylight for leisure. Here‚Äôs a tongue-in-cheek (and dystopian) idea: instead of changing clocks, Generative Reality could adjust how the world looks based on personal preference. Prefer Daylight Savings? Your headset could render the world as if it‚Äôs an hour ahead.</p> <h4 id="generative-ui"><strong>Generative UI</strong></h4> <p>A much more practical use case for native image generation is <a href="https://uxdesign.cc/an-introduction-to-generative-uis-01dcf6bca808" rel="external nofollow noopener" target="_blank">‚ÄúGenerative UI‚Äù</a> ‚Äî using generative models to create adaptive, context-aware user interfaces.</p> <p>Generative UI can be used to create personalized user interfaces that adapt to the user‚Äôs needs and preferences in real-time. For example, if a user is booking a business trip via Airbnb, the generative UI can automatically adjust the layout, to focus more on commute time to the downtown, or the speed of Wi-Fi at the hotel. On the other hand, family vacation booking might look more like existing Airbnb interfaces, with greater emphasis on family-friendly amenities, safety features, and cost comparisons as shown below. Unlike ‚ÄúGenerative Reality‚Äù, this is not a pipe dream, and is already supported to a certain extent by tools like Vercel‚Äôs <a href="https://sdk.vercel.ai/docs/ai-sdk-ui/generative-user-interfaces" rel="external nofollow noopener" target="_blank">AI SDK</a>.</p> <div class="container"> <div class="row justify-content-center"> <div class="col-md-10"> <div style="position: relative;"> <img-comparison-slider hover="hover" value="60"> <figure slot="first"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post3/airbnb-family-480.webp 480w,/assets/img/post3/airbnb-family-800.webp 800w,/assets/img/post3/airbnb-family-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/post3/airbnb-family.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure slot="second"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post3/airbnb-business-480.webp 480w,/assets/img/post3/airbnb-business-800.webp 800w,/assets/img/post3/airbnb-business-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/post3/airbnb-business.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </img-comparison-slider> <div class="caption-overlay">Generative UI for two different usecases: Family Travel &amp; Business Travel.</div> </div> </div> </div> </div> <p><br></p> <h3 id="whats-next"><strong>What‚Äôs Next?</strong></h3> <p>In terms of immediate industry headwinds, companies like Adobe, Figma, and stock photo platforms like Shutterstock and Unsplash are likely to be affected the most by the rise of native image generation models.</p> <ul> <li> <p><strong>Adobe and Figma</strong> are obviously affected the most, as their tools are heavily used for graphic design, UI/UX design, mockups, and image editing. Even if the current models cannot fully replace the need for professional design tools completely, they will only get better at instruction following over time, and will replace the need for these tools amongst the more casual users and hobbyists.</p> </li> <li> <p>The other companies that face significant negative headwinds are stock photo hosting and sharing platforms like <strong>Shutterstock and Unsplash</strong>. As image generation keeps improving, the need for licensing stock photos will continue to diminish, and the role of these companies will be ‚Äúdiminished‚Äù to that of licensing images for training these models. However, the timeline of this happening is not clear, since other Image generation models like Midjourney and Stable Diffusion have been around for a while, and have not significantly disrupted this market yet.</p> </li> <li> <p>There‚Äôs also potentially positive headwinds for <strong>AR/VR</strong> companies like Meta, Apple, Samsung, and more, since these models can help with creating newer User interfaces that are likely to make the AR/VR experience more immersive and personalized.</p> </li> <li> <p><strong>Social media companies</strong> like Meta, Snapchat, and TikTok could also integrate these tools into their platforms to allow users to add more interactive filters to their stories and posts, increasing user engagement and retention.</p> </li> </ul> <h4 id="world-modeling-using-3d-data-and-video">World Modeling using 3D Data and Video</h4> <p>As far as the core LLM technology is concerned, the logical next step would be to train these multi-modal models on even larger datasets, and even more modalities like video, and 3D models. Companies like <a href="https://worldlabs.ai/blog" rel="external nofollow noopener" target="_blank">World Labs</a> are already working on 3D world generation using LLMs / Foundation Models, and future frontier models like <code class="language-plaintext highlighter-rouge">Gemini-3.0</code>, <code class="language-plaintext highlighter-rouge">Grok 4</code>, <code class="language-plaintext highlighter-rouge">GPT-6</code> and more will likely have support for autoregressive video and 3D world generation as well. The priors for this happening are quite high, due to the existence of several large datasets of videos and 3D models available on the internet:</p> <ul> <li> <strong>Video datasets</strong>: All the content in YouTube, TikTok, movies, and more</li> <li> <strong>3D datasets</strong>: Data from varioues sources like Lidar scans from autonomous vehicles, 3D assets in game engines like Unreal Engine, Unity, and Blender, and spatial reconstruction data from NeRFs (Neural Radiance Fields) deployed in smartphone cameras and other devices.</li> </ul> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬© Copyright 2025 Shashank Srikanth. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Last updated: April 02, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?70d799092f862ad98c7876aa47712e20"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/img-comparison-slider@8.0.6/dist/index.min.js" integrity="sha256-EXHg3x1K4oIWdyohPeKX2ZS++Wxt/FRPH7Nl01nat1o=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/swiper@11.0.5/swiper-element-bundle.min.js" integrity="sha256-BPrwikijIybg9OQC5SYFFqhBjERYOn97tCureFgYH1E=" crossorigin="anonymous"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>