<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> LLMs, Game Theory, and Market Dynamics | Shashank Srikanth </title> <meta name="author" content="Shashank Srikanth"> <meta name="description" content="Game theory of frontier LLMs"> <meta name="keywords" content="Metaflow, Shashank Srikanth, tech, blog, academic-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://talsperre.github.io/blog/2025/game-theory/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Shashank</span> Srikanth </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">GitHub Profile </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">LLMs, Game Theory, and Market Dynamics</h1> <p class="post-meta"> Created in February 16, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a> ¬† ¬∑ ¬† <a href="/blog/tag/llms"> <i class="fa-solid fa-hashtag fa-sm"></i> llms</a> ¬† <a href="/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> ai</a> ¬† <a href="/blog/tag/tech"> <i class="fa-solid fa-hashtag fa-sm"></i> tech</a> ¬† ¬∑ ¬† <a href="/blog/category/tech"> <i class="fa-solid fa-tag fa-sm"></i> tech</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="grok-3-and-other-llm-announcements"><strong>Grok 3 and other LLM announcements</strong></h2> <p>This week, Elon Musk announced that Grok 3 is going to be released on February 17th, 2025. One reason to keenly watch the Grok 3 launch is that it‚Äôs one of the first models that has been trained on orders of magnitude more compute than previous gen GPT-4 class models. Grok 3 was pre-trained on the <a href="https://www.tomshardware.com/tech-industry/artificial-intelligence/elon-musk-confirms-that-grok-3-is-coming-soon-pretraining-took-10x-more-compute-power-than-grok-2-on-100-000-nvidia-h100-gpus" rel="external nofollow noopener" target="_blank">Colossus supercluster</a>, which contains over 100,000 NVIDIA H100 GPUs. The success of Grok 3 and its performance on benchmarks will be a good indicator of whether the scaling laws surrounding pre-training of LLMs still hold. Grok 3‚Äôs release announcement has triggered a cascade of announcements across the LLM landscape. OpenAI announced that they will be launching GPT-4.5, codenamed Orion in a few weeks time, and <a href="https://arstechnica.com/ai/2025/02/sam-altman-lays-out-roadmap-for-openais-long-awaited-gpt-5-model/" rel="external nofollow noopener" target="_blank">GPT-5</a> a few months after that. Additionally, there‚Äôs <a href="https://techcrunch.com/2025/02/13/anthropics-next-major-ai-model-could-arrive-within-weeks/?guccounter=1&amp;guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&amp;guce_referrer_sig=AQAAAA7gdeeXoLhVq6eKujg65469N-Ep5I6Ul85jxeQneLRvQYJ30ivnje6AA0spHhCkSrfWmL3vn1iTwk-_If0xRPQyDL0lSMxB2cfbRhm8VPaSERmB1EB8qvF600GeXyRCyuAU27bxTmX-oY8IqsVbpcSxYfEZe09bacS8S3slhvnc" rel="external nofollow noopener" target="_blank">rumors</a> that Anthropic is also planning on releasing a new model right around the same timeline.</p> <div class="row justify-content-center"> <div class="col-md-4"> <div class="row mb-4"> <div class="jekyll-twitter-plugin"> <blockquote class="twitter-tweet"> <p lang="en" dir="ltr">Grok 3 release with live demo on Monday night at 8pm PT. <br><br>Smartest AI on Earth.</p>‚Äî Elon Musk (@elonmusk) <a href="https://twitter.com/elonmusk/status/1890958798841389499?ref_src=twsrc%5Etfw" rel="external nofollow noopener" target="_blank">February 16, 2025</a> </blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> </div> <div class="row mb-3"> <div class="jekyll-twitter-plugin"> <blockquote class="twitter-tweet"> <p lang="en" dir="ltr">OPENAI ROADMAP UPDATE FOR GPT-4.5 and GPT-5:<br><br>We want to do a better job of sharing our intended roadmap, and a much better job simplifying our product offerings.<br><br>We want AI to ‚Äújust work‚Äù for you; we realize how complicated our model and product offerings have gotten.<br><br>We hate‚Ä¶</p>‚Äî Sam Altman (@sama) <a href="https://twitter.com/sama/status/1889755723078443244?ref_src=twsrc%5Etfw" rel="external nofollow noopener" target="_blank">February 12, 2025</a> </blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> </div> </div> <div class="col-md-6"> <div class="jekyll-twitter-plugin"> <blockquote class="twitter-tweet"> <p lang="en" dir="ltr">Anthropic is shaking things up in the AI race! üöÄ Their new hybrid model offers a unique approach to reasoning, giving developers granular control over cost and speed. Will this be a game-changer? ü§î <br><br>Read our story: <a href="https://t.co/jBQslxiT16" rel="external nofollow noopener" target="_blank">https://t.co/jBQslxiT16</a><a href="https://twitter.com/hashtag/AI?src=hash&amp;ref_src=twsrc%5Etfw" rel="external nofollow noopener" target="_blank">#AI</a> <a href="https://twitter.com/hashtag/ArtificialIntelligence?src=hash&amp;ref_src=twsrc%5Etfw" rel="external nofollow noopener" target="_blank">#ArtificialIntelligence</a>‚Ä¶</p>‚Äî The Information (@theinformation) <a href="https://twitter.com/theinformation/status/1890484912748204381?ref_src=twsrc%5Etfw" rel="external nofollow noopener" target="_blank">February 14, 2025</a> </blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> </div> </div> <p>This is a drastic departure from the past two years, where we saw a lot of announcement of new models, but not many releases that represented a step change in capabilities. <a href="https://openai.com/index/gpt-4-research/" rel="external nofollow noopener" target="_blank">GPT-4</a> was launched in May 2023, and the only other major update from OpenAI was the launch of <a href="https://openai.com/index/hello-gpt-4o/" rel="external nofollow noopener" target="_blank">GPT-4o</a> in May 2024, and the launch of reasoning models like <a href="https://openai.com/index/introducing-openai-o1-preview/" rel="external nofollow noopener" target="_blank">o1</a>, and <a href="https://openai.com/index/openai-o3-mini/" rel="external nofollow noopener" target="_blank">o3</a>. Unlike the jump in capabilities from GPT-3.5 to GPT-4, the jump in model quality from GPT-4 to GPT-4o was relatively minor. The same applies to the other capability jumps like those seen in the jump from <a href="https://www.anthropic.com/news/claude-3-family" rel="external nofollow noopener" target="_blank">Claude Sonnet</a> to <a href="https://www.anthropic.com/claude/sonnet" rel="external nofollow noopener" target="_blank">Claude Sonnet 3.5</a>, and from Gemini 1.5 Pro to Gemini 2.0 Pro. Reasoning models like o1 and o3 did show significant improvements in benchmark performance, but they are not directly comparable to base LLMs like GPT-4 since they rely on inference time compute.</p> <blockquote> <p>Grok 3 was announced finally on 17th February 2025 and it is indeed a great model. The model is first on <a href="https://x.com/lmarena_ai/status/1891706264800936307" rel="external nofollow noopener" target="_blank">LMArena leaderboard</a> with a score of over <code class="language-plaintext highlighter-rouge">1400</code> and its performance on <a href="https://x.com/emollick/status/1891708599560253906" rel="external nofollow noopener" target="_blank">benchmarks</a> is comparable to OpenAI‚Äôs <code class="language-plaintext highlighter-rouge">o3</code> models. The scale of improvements is not as drastic as the jump from GPT-3 to GPT-4, but the time it took xAI to catch up with other labs is impressive.</p> </blockquote> <h3 id="why-the-rush-to-release-models-now">Why the rush to release models now?</h3> <p>What‚Äôs interesting is that all of these companies plan on releasing their new LLMs within a few weeks of each other. Training these models often takes several months, and the post-training, fine-tuning, and red-teaming adds a few more months to this process. Given the long lead times, it‚Äôs unlikely that all of these companies just happened to finish training their models at the same time. This in turn, suggests that most of these companies have been sitting on their best models for a while, and are only releasing them now due to competitive pressures.</p> <p>But why would they do this? In a highly competitive market, it makes total sense for companies to release their best models as soon as they are ready, capture as much market share as possible, and then iterate on the next version. This has been the oft-repeated strategy in the tech industry in the past, and was taken to its zenith by companies like Uber, and Airbnb. The typical playbook during the Zero Interest Rate Policy (ZIRP) era was to raise massive amounts of capital, utilize that capital to subsidize services, and capture as much market share as possible. To hone in on the point further:</p> <ul> <li>Uber first launched a beta/demo in San Francisco in 2010.</li> <li>By 2012, Uber was serving rides across the US and in multiple cities across Europe.</li> <li>By 2013, Uber was available in <a href="https://www.theguardian.com/news/2022/jul/15/embrace-the-chaos-a-history-of-ubers-rapid-expansion-and-fall-from-favour" rel="external nofollow noopener" target="_blank">74</a> cities and was valued at $3.5 billion.</li> </ul> <p>This paradigm of flooding the market with your best product using VC funding as soon it is ready doesn‚Äôt seem to apply to the LLM space. This is due to a unique set of circumstances and market circumstances like:</p> <ul> <li>High interest rates</li> <li>Competitive dynamics among companies due to the existence of knowledge distillation,</li> <li>And the possibility of recursive self-improvement of models which depend on acquiring more user data and training even larger models.</li> </ul> <p>Let‚Äôs explore each one of these in more detail.</p> <h2 id="high-interest-rates">High Interest Rates</h2> <p>During the ZIRP era, raising capital was easy - this capital could be deployed quickly to build out infrastructure, acquire users, and capture market share. However, the higher interest rates of the past few years have made it difficult to apply the same playbook. The foundational assumption of the earlier tech era companies was that the initial infrastructure, R&amp;D, and user acquisition costs would be high, but once a user was acquired, the marginal cost of serving that user would be low. This is what makes Google‚Äôs AdWords, Facebook‚Äôs marketplace / Ad network, and AWS‚Äôs compute services so profitable. However, the marginal cost of serving users in the LLM space is not low, and is estimated to be in multiple billions of dollars. Tn fact, the marginal cost of serving a user is significantly higher for models using inference time compute like <code class="language-plaintext highlighter-rouge">o1</code>, <code class="language-plaintext highlighter-rouge">o1 pro</code>, and <code class="language-plaintext highlighter-rouge">o3</code>, and is requiring companies to spend massively on data center and power generation. The costs are so high that OpenAI has to charge over $200 for their most premium <a href="https://openai.com/chatgpt/pricing/" rel="external nofollow noopener" target="_blank">subscription</a> which is the only way to access their latest o1 pro models.</p> <p>The return to a world where marginal costs matter once more implies that companies like OpenAI, Anthropic, Google, and others have to be more judicious in how they deploy their capital, and make sure that they are able to recoup their costs.</p> <h2 id="competitive-market-dynamics">Competitive market dynamics</h2> <p>Beyond the financial constraints, two key competitive dynamics define how companies navigate the LLM landscape:</p> <ul> <li>User acquisition and training data acquisition</li> <li>Knowledge distillation</li> </ul> <p>Both these dynamics are interrelated, and result in a game theoretic situation where entities are incentivized to act in a certain way depending on what they think other entities will do, and the other entities‚Äô actions.</p> <h3 id="user-acquisition-and-training-data-acquisition">User Acquisition and Training Data Acquisition</h3> <p>LLMs are trained on vast amounts of data, and if the <a href="https://arxiv.org/abs/2203.15556" rel="external nofollow noopener" target="_blank">scaling laws</a> hold, then training larger models on more data will result in better models. This implies that the entity that acquires the most users, and subsequently the most data, will be able to train the best models, all other things being equal. Thus, entities are incentivized to acquire as many users as possible, which can be done by either releasing new state-of-the-art models that are significantly better than the competition, or by offering the best models at a lower price point.</p> <h3 id="knowledge-distillation">Knowledge Distillation</h3> <p>On the one hand, releasing new frontier models has positive network effects for the entity releasing the model, its benefits are short-lived. This is due to the existence of <strong>Knowledge Distillation</strong>, which allows other entities to train smaller models that have similar performance to the original entity‚Äôs frontier model at a fraction of the cost. Distillation is very common in the LLM space and is frequently used by companies to train smaller models that can serve users at a lower price point. Some examples of this include DeepSeek‚Äôs V3 model, LLama 7B model, OpenAI‚Äôs GPT-4o mini, and Google‚Äôs Gemini 2.0 Flash.</p> <h4 id="what-is-knowledge-distillation">What is Knowledge Distillation?</h4> <p><strong>Knowledge Distillation</strong> was first proposed in its modern form by Geoffrey Hinton, Oriol Vinyals, and Jeff Dean in their 2015 paper <a href="https://arxiv.org/abs/1503.02531" rel="external nofollow noopener" target="_blank">Distilling the Knowledge in a Neural Network</a>. It‚Äôs a fairly old concept, at least by modern ML standards, and at its core, is just a process of training a smaller model to mimic the behavior of a larger model. While a fairly simple concept, the implications of knowledge distillation being possible are profound. Here‚Äôs another way of looking at knowledge distillation:</p> <ul> <li>Say, you have a large model \(X\) trained on a dataset \(D\), with a certain performance metric \(Y\).</li> <li>Now, you want to train a much smaller model \(X'\) with smaller compute budget that has roughly similar performance to \(X\).</li> <li>Instead of training \(X'\) on \(D\) directly, you instead train it on the predictions of \(X\) on \(D\).</li> <li>This surprisingly results in better performance than training \(X'\) on \(D\) directly.</li> </ul> <p>Here‚Äôs a more authoritative excerpt from a 2021 paper ‚ÄúDoes Knowledge Distillation Really Work?‚Äù by <a href="https://arxiv.org/abs/2106.05237" rel="external nofollow noopener" target="_blank">Stich et al.</a>:</p> <blockquote> <p>Large, deep networks can learn representations that generalize well. While smaller, more efficient networks lack the inductive biases to find these representations from training data alone, they may have the capacity to represent these solutions. Influential work on knowledge distillation argues that Bucila et al. ‚Äúdemonstrate convincingly that the knowledge acquired by a large ensemble of models <strong>[the teacher]</strong> can be transferred to a single small model <strong>[the student]</strong>‚Äù. Indeed this quote encapsulates the conventional narrative of knowledge distillation: a student model learns a high-fidelity representation of a larger teacher, enabled by the teacher‚Äôs soft labels.</p> </blockquote> <p>Several mechanisms for knowledge distillation have been proposed in literature such as using a distillation loss to capture the difference between the teacher and student predictions, using novel architectures and distillation loss functions that minimize the difference between the feature activations of the teacher and student, and more. Knowledge Distillation itself can be applied at various stages of model training:</p> <ul> <li>In an offline setting, where the teacher model is trained first, and then the student model is trained on the teacher‚Äôs predictions.</li> <li>In an online setting, where the teacher and student models are trained simultaneously in an end-to-end manner.</li> <li>Self-distillation, where the teacher and student models are the same, and the deeper layers of the model are used to train the shallower layers.</li> </ul> <p>This <a href="https://neptune.ai/blog/knowledge-distillation" rel="external nofollow noopener" target="_blank">blog post</a> goes into greater detail on the various types of knowledge distillation, and specific case studies.</p> <h4 id="why-does-knowledge-distillation-matter">Why does knowledge distillation matter?</h4> <p>As explained earlier, the entity releasing the best model first only has a short window of time to reap the economic rewards from it, before others distill from it and release smaller models that have similar performance. The plot below shows how the compute budget required to train a new non-reasoning frontier LLM has been increasing exponentially over the past few years. The compute budget for Grok 3 pre-training is estimated to be around \(10\times\) that of GPT-4, and that of GPT-4 itself was rouhgly \(50\times\) that of GPT-3. However, once a new frontier LLM is released, it only takes roughly a year at max for other companies to release much smaller models that have similar performance to the previous frontier LLM.</p> <div class="l-page"> <iframe src="/assets/d3/post2/llm_scaling.html" width="100%" height="720px" style="border: 1px dashed grey; overflow: hidden;"> </iframe> </div> <p>For instance, DeepSeek‚Äôs V3 model, a highly capable GPT-4 level model, was trained on just a cluster of 2048 H800 GPUs, which is a fraction of the GPUs used to train GPT-4. To achieve this drastic reduction in compute costs, DeepSeek made several innovations such as:</p> <ul> <li>Leveraging Nvidia‚Äôs assembly level PTX programming directly to bypass the limitations of the CUDA API, making it easier to achieve better GPU interconnect bandwidth.</li> <li>Using a novel multi-head latent attention layer that uses low rank compression of keys and values, to reduce memory footprint.</li> <li>Using DeepSeekMoE with support for hybrid routing, dynamic load balancing, and sequence wise balancing, to improve the efficiency of the model.</li> </ul> <p>However, DeepSeek‚Äôs V3 model also benefited from the fact that it was able to distill from other frontier LLMs like OpenAI‚Äôs GPT-4o, if OpenAI‚Äôs / Microsoft‚Äôs <a href="https://www.theverge.com/news/601195/openai-evidence-deepseek-distillation-ai-data" rel="external nofollow noopener" target="_blank">claims</a> are to be believed. Thus, there is a game theory dynamic at play where entities are incentivized to sit on their best models until the last possible moment, and only release them when forced by market conditions.</p> <h2 id="recursive-self-improvement">Recursive self-improvement</h2> <p>I personally don‚Äôt ascribe to the view that LLMs will achieve recursive self-improvement anytime soon, but it‚Äôs worth mentioning as another factor that could be driving the strategy of releasing frontier models amongst entities. The idea of recursive self-improvement is that once an AI system reaches a certain threshold of capability, it can then use that capability to improve itself further, and so on. Think of it as a positive feedback loop where the AI uses its current capabilities to improve itself exponentially, resulting in an Artificial Superintelligence (ASI). For entities ascribing to this view, there is no incentive to release new frontier models unless it is already an ASI due to two reasons:</p> <ul> <li>Every small improvement in the model‚Äôs current capabilities could be used to improve the model further, and releasing it to the public would allow other‚Äôs to reverse engineer the improvements or distill from it. In order to truly reap the economic rewards of recursive self-improvement, it only makes sense to release the model once it is already an ASI.</li> <li>For entities concerned about ‚ÄúAI safety‚Äù or ‚ÄúAI alignment‚Äù, releasing a model that is capable of recursive self-improvement could be catastrophic if the model is not aligned with human values. Thus, they would rather release models only after they have ensured that the model is aligned with human values, and is safe to use.</li> </ul> <p>These values seem to align with strategies of companies like <a href="https://ssi.inc/" rel="external nofollow noopener" target="_blank">Safe Superintelligency (SSI)</a>, and <a href="https://www.anthropic.com/" rel="external nofollow noopener" target="_blank">Anthropic</a>,</p> <h2 id="how-is-this-related-to-game-theory"><strong>How is this related to game theory?</strong></h2> <p>The competing dynamics of user/data acquisition, knowledge distillation, and recursive self-improvement can be modelled using game theory. There are several competitors, which have partial information about each other‚Äôs capabilities, and are incentivized to act in a way that maximizes their own utility, but not necessarily the utility of the group. The utility for various entities is different, but at a high level, it can be thought of as maximizing shareholder value. There are several other factors at play as well, some of which can be confounding, such as regulatory constraints, ethical considerations, and more. Thus, any attempt to model this using a simple game theoretic framework would be an oversimplification, but it‚Äôs still a good approximation of the competitive dynamics at play - and can be used to answer the original question of why release of frontier models are typically clustered together.</p> <h3 id="modeling-the-llm-space-as-a-two-stage-coordination-game">Modeling the LLM space as a two-stage coordination game</h3> <p>The forces of user/data acquisition and knowledge distillation are at odds with each other, and can be modeled as a two-stage coordination game. Both stages can be modelled as a coordination game, but the strategies and payoffs are different for each stage.</p> <h4 id="what-is-a-coordination-game">What is a coordination game?</h4> <p>A <a href="https://en.wikipedia.org/wiki/Coordination_game" rel="external nofollow noopener" target="_blank">Coordination game</a> is a type of game where players benefit from making the same or compatible choices. Assuming rationality, the key characteristic of coordination games is that there are multiple equilibrium, and the main challenge lies in selecting the equilibrium that is most beneficial for all players. Selecting the optimal equilibrium is often difficult due to the lack of communication between players, or having only partial information about the other players‚Äô strategies. A <a href="https://en.wikipedia.org/wiki/Focal_point_(game_theory)" rel="external nofollow noopener" target="_blank">Schelling point</a> (or focal point) is a solution that players tend to choose in the absence of communication because it appears naturally prominent or intuitive. Depending on the payoffs, the Schelling point is often the optimal equilibrium for all players, but in other cases, it might be worse for all entities involved.</p> <p>The competitive dynamics in the LLM space can be modeled as a coordination game because all entities can be assumed to be rational, and act in their own self-interest, and only have partial information about the other‚Äôs entities strategies. In other words, in the absence of direct communication, the entities will have to make decisions based on their own self-interest, and also based on an understanding of what the other entities are likely to do.</p> <h4 id="stag-hunt-and-prisoners-dilemma-games">Stag Hunt and Prisoner‚Äôs Dilemma games</h4> <p>More concretely, the competitive dynamics can be modeled as a two-stage coordination game, where the first stage is a ‚ÄúStag Hunt‚Äù game, and the second stage is a ‚ÄúPrisoner‚Äôs Dilemma‚Äù game. The videos below provide a good overview of the <a href="https://en.wikipedia.org/wiki/Stag_hunt" rel="external nofollow noopener" target="_blank">Stag Hunt</a> and <a href="https://en.wikipedia.org/wiki/Prisoner%27s_dilemma" rel="external nofollow noopener" target="_blank">Prisoner‚Äôs Dilemma</a> games, and how the payoffs are different for each game. The difference in payoffs between the two games also results in a difference in strategies that players are incentivized to take to maximize their utility.</p> <div class="row justify-content-center w-100"> <div class="col-sm-6"> <iframe width="100%" height="400" src="https://www.youtube.com/embed/oQ3KmcjwuKU" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" style="border-radius: 8px; box-shadow: 0 2px 8px rgba(0, 0, 0, 0.15);"> </iframe> </div> <div class="col-sm-6"> <iframe width="100%" height="400" src="https://www.youtube.com/embed/jr2b0aZfOZQ" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" style="border-radius: 8px; box-shadow: 0 2px 8px rgba(0, 0, 0, 0.15);"> </iframe> </div> </div> <blockquote> <p>Videos were created by the <code class="language-plaintext highlighter-rouge">o1</code> model using <a href="https://www.manim.community/" rel="external nofollow noopener" target="_blank">manim</a> and the prompts were created by yours truly.</p> </blockquote> <p>In the context of frontier LLM releases, the first stage can be represented as a <strong>Stag Hunt</strong> game, where entities are incentivized to hold out on releasing their best models for as long as market conditions allow. Sitting on the best model allows entities to capture as much value out of their existing models and infrastructure as possible, and also provides enough time to train the next frontier model. Releasing a new model does provide a guaranteed short-term competitive advantage over other entities, but in the long run, the benefits are not as clear, since it won‚Äôt take long for other entities to catch up via distillation or other means. There are two equilibriums possible in this stage:</p> <ul> <li> <strong>Equilibrium 1</strong>: All entities hold out on releasing their best models, and only release them when forced by market conditions.</li> <li> <strong>Equilibrium 2</strong>: One entity releases their best model, and forces all other entities to release their best models as well. This implies that all entities release their best models at the same time.</li> </ul> <div class="row justify-content-center"> <div class="col-sm-8"> <iframe width="100%" height="500" src="/assets/d3/post2/stag_hunt_payoff_matrix.html" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" style="border-radius: 8px; box-shadow: 0 2px 8px rgba(0, 0, 0, 0.15);"> </iframe> </div> </div> <p>The payoff matrix for the Stag Hunt game in the LLM space is shown above. Due to the existence of partial communication among the various entities, and partial trust in the other entities‚Äô strategies, the Schelling point in this case is to hold out on releasing the best model, and it also happens to be the optimal equilibrium for all entities involved. This is exactly what we are witnessing in the last few months. However, as soon as one entity releases or announces a plan to release a new frontier model, the market dynamics change, and the second stage of the game begins.</p> <p>The second stage of the game has different payoff structure, and can be modelled as a <strong>Prisoner‚Äôs Dilemma</strong> game. In this game, entities are forced to release their best models to prevent other entities from capturing market share at their expense. Given the nature of the payoffs, there is only one equilibrium in this game:</p> <ul> <li> <strong>Equilibrium 1</strong>: All entities are forced to release their best models as soon as one entity announces a plan to release a new frontier model.</li> </ul> <p>Unlike the Stag Hunt game, there is only one Nash equilibrium in the Prisoner‚Äôs Dilemma which is <code class="language-plaintext highlighter-rouge">(Defect, Defect)</code> (or <code class="language-plaintext highlighter-rouge">(Release, Release)</code> to be more specific). Defection is the dominant strategy in this game, and since all entities are aware of this, the Schelling point also becomes <code class="language-plaintext highlighter-rouge">(Defect, Defect)</code>. However, unlike the Stag Hunt game, the Schelling point is not the pareto optimal and is not the best outcome for all entities involved. Taken to its extreme, this bodes poorly for the frontier LLM providers, as they will have to constantly raise new capital to expand their infrastructure, and train even larger models to stay ahead of the competition. Additionally, there are other entities which aren‚Äôt interested in developing frontier LLMs, but nonetheless benefit from the existence of these models due to knowledge distillation. In an ideal world, all entities would be better off if they held out on releasing their best models, but the payoffs are such that the dominant strategy is worse for all entities involved. The payoff matrix for the Prisoner‚Äôs Dilemma game in the LLM space is shown below.</p> <div class="row justify-content-center"> <div class="col-sm-8"> <iframe width="100%" height="500" src="/assets/d3/post2/prisoners_dilemma_payoff_matrix.html" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" style="border-radius: 8px; box-shadow: 0 2px 8px rgba(0, 0, 0, 0.15);"> </iframe> </div> </div> <p>We have seen dynamic play out just a month ago, when OpenAI made <a href="https://openai.com/index/openai-o3-mini/" rel="external nofollow noopener" target="_blank">o3 mini</a> available for free to all users, as soon as DeepSeek announced their R1 model. We are seeing the same dynamic play out again, with OpenAI and Anthropic have announced their intent to release new frontier models in the coming weeks, due to Grok 3‚Äôs imminent release. The visualization‚Äôs below show how the payoffs and the dominant strategies change between the two stages of the game.</p> <h2 id="implications"><strong>Implications</strong></h2> <p>The above game theoretic model provides a good approximation of the competitive landscape in the LLM space, but is not a perfect model. It suffers from some drawbacks, chiefly:</p> <ul> <li>While most entities can be assumed to be rational and act in their own self-interest, i.e. maximize shareholder value, the means to achieve this may vary. For instance, companies like Meta and Amazon benefit from LLMs becoming commoditized since they are aggregators, and can thus release new models as soon as they are ready.</li> <li>If releasing frontier models is indeed the dominant strategy, then this might result in an arms race, where entities are incentivized to constantly train larger models to stay ahead of the competition. However, this means less time and resources are spent on important safety aspects like model alignment, and ethical considerations. <strong>Note</strong>: This is a highly controversial topic, and I am not suggesting that safety either be prioritized or deprioritized, but it‚Äôs a tradeoff that entities will have to make.</li> <li>Releasing becoming the dominant strategy implies that frontier LLMs will become commoditized. This means that the economic rewards from releasing new models not only accrue to other competitors, but also to entities in other industries that can leverage these models to improve their own workflows. Thus, there is an argument to be made that policies should incentivize entities to release frontier models as soon as they are ready, for the ‚Äúgreater good‚Äù.</li> </ul> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬© Copyright 2025 Shashank Srikanth. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Last updated: April 01, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?70d799092f862ad98c7876aa47712e20"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>